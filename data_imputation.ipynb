{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265d4e42",
   "metadata": {},
   "source": [
    "### Data imputation\n",
    "About data imputation you have two options in order to replace NaN values:\n",
    "1. Get real values and replace manually\n",
    "2. Imputation model (obviously consider that values predicted can not be aligned completely with reality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797fa94",
   "metadata": {},
   "source": [
    "<u>NOTE:</u>\n",
    "\n",
    "* Following udf's are focused on testing MICE forest model imputational method and finally use it in order to replace nan values in dataframe.\n",
    "\n",
    "* Note that in this case has been used for macreconomic model which have categorical and not categorical columns.\n",
    "  Finally, in this udf 'mice_forest_geo', the dataset is splitted by geo value, imputed nan and then merge all part been splitted. You can modify udf if you want make imputation with all df without splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8fd02",
   "metadata": {},
   "source": [
    "**SetUp**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1b99b",
   "metadata": {},
   "source": [
    "<u>Install libraries</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4136f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61525646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install miceforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d9388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ee8e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pandas pyarrow miceforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbe95581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library installation (check it out to see how it works)\n",
    "# !pip install git+https://github.com/AnotherSamWilson/miceforest.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7defd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install miceforest --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b263ac3",
   "metadata": {},
   "source": [
    "*(You can get an error about compatibility beetwen miceforest and pyarrow)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175e3f9",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73af4de",
   "metadata": {},
   "source": [
    "**<u>Import libraries</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9a561d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "#- sklearn imputation libraries\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# - sklearn general using\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Miceforest (MICE)\n",
    "import miceforest as mf\n",
    "from miceforest import ImputationKernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdeda4",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355cf034",
   "metadata": {},
   "source": [
    "**<U>Sum up dataframe dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53deec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_up_count_values_df(df):\n",
    "    '''\n",
    "    Sum up:\n",
    "    - dataframe size\n",
    "    - count total values not nan\n",
    "    - count total values nan\n",
    "    '''\n",
    "    print(f'Total df values (non-numeric and numeric): {df.size}')\n",
    "    print(f'Total values (numeric): {df.select_dtypes(include=[np.number]).count().sum().sum()}')\n",
    "    print(f'Total values (numeric): {df.select_dtypes(include=[np.number]).isna().sum().sum()}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00817c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_up_count_values = sum_up_count_values_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8c204",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data correlation beetwen each couple of columns\n",
    "# g = sns.PairGrid(df_test_rep)\n",
    "# g.map(plt.scatter, s=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5135f4b",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3b258",
   "metadata": {},
   "source": [
    "**<U>Sklearn imputational methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ce8ae",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa31e6",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5edf7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_imputation_knn(df, test_column = 'HICP(%)', grouping_col = 'geo', size_test=0.2):\n",
    "    \n",
    "    # Suggestion to use train test: with enough quantity of data\n",
    "    \n",
    "    '''\n",
    "    KNN made by each distinct values of 'geo' column\n",
    "    '''\n",
    "    \n",
    "    col_to_test = test_column\n",
    "    \n",
    "    true_values_list = []\n",
    "    imputed_values_list = []\n",
    "    \n",
    "    groups = df.groupby(grouping_col, group_keys=False)\n",
    "\n",
    "    def impute_group(group):\n",
    "        train, test = train_test_split(group.dropna(subset=[col_to_test]), test_size=size_test, random_state=42)\n",
    "        test_copy = test.copy()\n",
    "        missing_rows = test_copy.sample(frac=0.1).index\n",
    "        true_values = test_copy.loc[missing_rows, col_to_test]\n",
    "        test_copy.loc[missing_rows, col_to_test] = np.nan\n",
    "\n",
    "        numerical_columns = train.select_dtypes(include=[np.number]).columns\n",
    "        imp = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "        imp.fit(train[numerical_columns])\n",
    "        imputed_test = imp.transform(test_copy[numerical_columns])\n",
    "        imputed_test_df = pd.DataFrame(imputed_test, columns=numerical_columns, index=test.index)\n",
    "\n",
    "        true_values_list.extend(true_values)\n",
    "        imputed_values_list.extend(imputed_test_df.loc[missing_rows, col_to_test])\n",
    "\n",
    "        imputed_group = pd.concat([train, imputed_test_df], sort=False)\n",
    "\n",
    "        return imputed_group\n",
    "\n",
    "    imputed_df = groups.apply(impute_group).reset_index(drop=True)\n",
    "\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'True Values': true_values_list,\n",
    "        'Imputed Values': ['{:.1f}'.format(value) for value in imputed_values_list]\n",
    "    })\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(true_values_list, imputed_values_list))\n",
    "\n",
    "    range_of_data = df[test_column].max() - df[test_column].min()\n",
    "    percentage_error = (rmse / range_of_data) * 100\n",
    "    print(f'RMSE percentage_error (min,max): {percentage_error:.2f}')\n",
    "    \n",
    "    display(comparison_df)\n",
    "\n",
    "    return imputed_df, comparison_df, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df, comparison_df, rmse = iterative_imputation_knn(df, test_column = 'HICP(%)', grouping_col = 'geo', size_test=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70b940",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a4640",
   "metadata": {},
   "source": [
    "**'BayesianRidge' or 'LinearRegression'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31946c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_imputation_sklearn(df, estimator_choice='BayesianRidge', col_to_test = 'HICP(%)', grouping_col = 'geo'):\n",
    "    \n",
    "    '''\n",
    "    Impute missing values using iterative imputer.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - estimator_choice: Either 'BayesianRidge' or 'LinearRegression'\n",
    "    \n",
    "    Returns:\n",
    "    imputed_df, comparison_df, rmse\n",
    "    '''\n",
    "    \n",
    "    # Check estimator choice\n",
    "    if estimator_choice not in ['BayesianRidge', 'LinearRegression']:\n",
    "        raise ValueError(\"estimator_choice must be either 'BayesianRidge' or 'LinearRegression'\")\n",
    "\n",
    "    # Select column to check\n",
    "    \n",
    "    true_values_list = []\n",
    "    imputed_values_list = []\n",
    "    categorical_values_list = []\n",
    "\n",
    "    # Group by 'geo' column\n",
    "    groups = df.groupby(grouping_col)\n",
    "    \n",
    "    # Model used\n",
    "    print('BayesianRidge model')\n",
    "\n",
    "    # Define a function to impute missing values within each group\n",
    "    def impute_group(group):\n",
    "        subset_group = group.dropna(subset=[col_to_test]).copy()\n",
    "        true_values = subset_group.sample(frac=0.1)[col_to_test]\n",
    "        subset_group.loc[true_values.index, col_to_test] = np.nan\n",
    "        numerical_columns = subset_group.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        # -- model decided to use in udf setting\n",
    "        if estimator_choice == 'BayesianRidge':\n",
    "            imp = IterativeImputer(estimator=BayesianRidge(), max_iter=30, random_state=0)\n",
    "        else:\n",
    "            imp = IterativeImputer(estimator=LinearRegression(), max_iter=30, random_state=0)\n",
    "            \n",
    "        subset_group[numerical_columns] = imp.fit_transform(subset_group[numerical_columns])\n",
    "        \n",
    "        categorical_columns = subset_group.select_dtypes(exclude=[np.number]).columns\n",
    "        # --\n",
    "        \n",
    "        for idx in true_values.index:\n",
    "            true_values_list.append(true_values[idx])\n",
    "            imputed_values_list.append(subset_group.loc[idx, col_to_test])\n",
    "            categorical_values_list.append(group.loc[idx, categorical_columns])\n",
    "\n",
    "        return subset_group\n",
    "\n",
    "    imputed_df = groups.apply(impute_group).reset_index(drop=True)\n",
    "    \n",
    "    # -- df comparing values\n",
    "    comparison_df = pd.DataFrame({\n",
    "        **pd.DataFrame(categorical_values_list),\n",
    "        'True Values': true_values_list,\n",
    "        'Imputed Values': ['{:.1f}'.format(value) for value in imputed_values_list]\n",
    "    })\n",
    "    \n",
    "    # -- Evaluate model\n",
    "    rmse = np.sqrt(mean_squared_error(true_values_list, imputed_values_list))\n",
    "    range_of_data = df[col_to_test].max() - df[col_to_test].min()\n",
    "    percentage_error = (rmse / range_of_data) * 100\n",
    "    \n",
    "    print(f'MEAN true values: {np.mean(true_values_list):.2f}')\n",
    "    print(f'MEAN imputed values: {np.mean(imputed_values_list):.2f}')\n",
    "    print(f'RMSE percentage_error (min,max): {percentage_error:.2f}')\n",
    "    \n",
    "    display(comparison_df)\n",
    "\n",
    "    return imputed_df, comparison_df, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15fb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df, comparison_df, rmse = iterative_imputation_sklearn(df, estimator_choice='BayesianRidge', col_to_test = 'HICP(%)', grouping_col = 'geo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aceefec",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140f76d",
   "metadata": {},
   "source": [
    "**<u>MICE model, following tested two approach</u>**\n",
    "* All dataset\n",
    "* Subgrouped by 'geo', for two reason:\n",
    "    * we can not removed bias from years just selecting one year caused by low data\n",
    "    * identifying more comparable countries\n",
    "    \n",
    "***To take into consideration***: mice models used consider also outliers values, because our goal si to identify values that\n",
    "    could be consistent with period and country reference. Decision to consider outliers has been done\n",
    "    because outliers are not wrong values but correct one and second because if we remove potentially\n",
    "    value predicted in same outlier row will be not realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d3916",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975466c7",
   "metadata": {},
   "source": [
    "**1) Train and test model**\n",
    "\n",
    "<u>*!!! Suggestion to use train test: with enough quantity of data*</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define by yourself: df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87c31af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def mice_train_test(X, y):\n",
    "    \n",
    "    '''\n",
    "    # Make sure your 'object' columns are converted to 'category' type\n",
    "    (category are more dreadable for ml models)\n",
    "    \n",
    "    Steps model:\n",
    "    1) Imputation MICE for X train (saved in 'X_train_imputed')\n",
    "    2) Train LR model in 'X_train_imputed'\n",
    "    3) Imputation MICE for X test (saved in 'X_test_imputed') \n",
    "    4) predictions variable: model predict 'X_test_imputed'\n",
    "    Results beetwen y_test, predictions: rmse, r2, adjusted_r2\n",
    "    \n",
    "    ====================\n",
    "   \n",
    "    - Select original dataframe\n",
    "    - Split in: X, y\n",
    "    - test_size setted: 0.2 (suggestion to use train test: with enough quantity of data)\n",
    "    \n",
    "    Note: make sure you have enough nan value to make MICE interesting to use\n",
    "    '''\n",
    "\n",
    "    # Train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Kernel initialization for miceforest training\n",
    "    kernel_train = mf.ImputationKernel(X_train, save_all_iterations=True, random_state=0)\n",
    "\n",
    "    # impute the missing values in the training set\n",
    "    kernel_train.mice(3)\n",
    "\n",
    "    # recover the imputed dataset\n",
    "    X_train_imputed = kernel_train.complete_data()\n",
    "    X_train_df = pd.DataFrame(X_train_imputed)\n",
    "\n",
    "    # Train imputed data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "\n",
    "    # We input the missing values in the test set using the same information from the training set\n",
    "    kernel_test = mf.ImputationKernel(X_test, save_all_iterations=True, random_state=0)\n",
    "    kernel_test.mice(3)\n",
    "\n",
    "    # recover the imputed dataset\n",
    "    X_test_imputed = kernel_test.complete_data()\n",
    "    X_test_df = pd.DataFrame(X_test_imputed)\n",
    "\n",
    "    # Root Mean Squared Error:  calculate the Model Standard Quadratic Error on the test set\n",
    "    predictions = model.predict(X_test_imputed)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    print(f'Root Mean Squared Error: {rmse}')\n",
    "\n",
    "    # R2\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(f'R-squared: {r2}')\n",
    "\n",
    "    # R2 Adjusted\n",
    "    n = y_test.shape[0] # the number of observations\n",
    "    p = X_test.shape[1] # the number of predictors\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    adjusted_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "    print(f'Adjusted R-squared: {adjusted_r2}')\n",
    "    \n",
    "    return rmse, r2, adjusted_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9f2d9",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7342044",
   "metadata": {},
   "source": [
    "**2) Imputation using all dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "751f4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one: made by all data\n",
    "def mice_imputation_test(df, col_to_test = 'HICP(%)', cols_to_category=['geo', 'country', 'year']):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    --> test one: made using all data for imputation\n",
    "     \n",
    "    =================\n",
    "    \n",
    "    Impute need:\n",
    "    \n",
    "    Select original dataframe (with nan values)\n",
    "    \n",
    "    1) Select df with original values\n",
    "    1) Select columns to test. That values will be randomly converted to nan and tested with predictions\n",
    "    2) All nan values predicted will be compared with real\n",
    "    \n",
    "    Note: Make sure you don't have empty values in columns: 'geo', 'country', 'year'\n",
    "    \n",
    "    =================\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    - kds: imputed data\n",
    "    - imputed_df: dataframe imputed\n",
    "    - comparison_df: dataframe with true values and imputed values\n",
    "    - rmse: Root Mean Squared Error (RMSE) is calculated between the real and imputed values.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If columns are not category yet, convert them\n",
    "    df_mf = df.copy()\n",
    "    df_mf[cols_to_category] = df_mf[cols_to_category].astype('category')\n",
    "    \n",
    "    # Create a copy of the DataFrame and select a fraction of the known data to be imputed\n",
    "    df_mf = df.dropna(subset=[col_to_test]).copy()\n",
    "    true_values = df_mf.sample(frac=0.1)[col_to_test]\n",
    "    df_mf.loc[true_values.index, col_to_test] = np.nan\n",
    "    \n",
    "    # --- MICE model creating\n",
    "    kds = mf.ImputationKernel(df_mf, random_state=42)\n",
    "    \n",
    "    # --- Tuning parameters\n",
    "    optimal_parameters, losses = kds.tune_parameters(dataset=0, optimization_steps=10)\n",
    "    kds.mice(1, variable_parameters=optimal_parameters)\n",
    "    # print(optimal_parameters)\n",
    "    # ---\n",
    "    \n",
    "    # --- Manual setting\n",
    "    # n_estimators, are number of tree. So, define it considering how mauch data your df contains\n",
    "    # kds.mice(iterations=20, n_estimators=10, device='gpu') # verbose=2\n",
    "\n",
    "\n",
    "    # --- Complete the imputed data\n",
    "    imputed_df = kds.complete_data()\n",
    "\n",
    "    # Recovers the imputed values\n",
    "    imputed_values = imputed_df.loc[true_values.index, col_to_test]\n",
    "\n",
    "    # --- Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, imputed_values))\n",
    "\n",
    "    # Calculate the percentage error\n",
    "    range_of_data = df[col_to_test].max() - df[col_to_test].min()\n",
    "    percentage_error = (rmse / range_of_data) * 100\n",
    "    print(f'RMSE percentage_error (min,max): {percentage_error:.2f}')\n",
    "\n",
    "    # --- Create a DataFrame with real and imputed values\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'True Values': true_values,\n",
    "        'Imputed Values': ['{:.1f}'.format(value) for value in imputed_values]\n",
    "    })\n",
    "    \n",
    "    display(comparison_df)\n",
    "\n",
    "    return imputed_df, comparison_df, rmse, kds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c701dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataframe (df_test_rep)\n",
    "imputed_df, comparison_df, rmse, kds = mice_imputation_test(df, col_to_test = 'HICP(%)', cols_to_category=['geo', 'country', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf45315",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot compare the distribution by data and data with imputation\n",
    "kds.plot_imputed_distributions(wspace=0.2,hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd625d9",
   "metadata": {},
   "source": [
    "<!-- <u>Model (use 'geo' reference)</u> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58881cb",
   "metadata": {},
   "source": [
    "**3) Imputation using subgroups made by geo column values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29f47bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test two: made by each 'geo'\n",
    "def mice_forest_grouped(df, col_to_test='HICP(%)', grouping_column='geo', other_cat_col='country'): # other_cat_column (no year)\n",
    "    \n",
    "    '''\n",
    "    --> test two: imputation made by each 'geo' distinct value\n",
    "     \n",
    "    =================\n",
    "    \n",
    "    This function allows to impute the missing data in a dataframe taking into account\n",
    "    the subgroups formed by the \"geo\" column, providing an estimate of the accuracy of\n",
    "    the imputation and returning the imputed data.\n",
    "    \n",
    "    Decision has been made because in this way prediction will take into cosnderation countries similarities.\n",
    "      \n",
    "    ============\n",
    "    \n",
    "    Imput needs:\n",
    "    - df: original dataframe\n",
    "    - col_to_test: column to use fo testing prediction\n",
    "    - geo_column: geo column in dataframe\n",
    "    - country_column: country column in dataframe\n",
    "    \n",
    "    ============\n",
    "    \n",
    "    Results:\n",
    "    - final_df: all imputed dataframes are combined into a single dataframe.\n",
    "    - rmse: Root Mean Squared Error (RMSE) is calculated between the real and imputed values.\n",
    "    - comparison_df: a dataframe is created to compare the real and imputed values.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    df_geo_list = df[geo_column].unique()\n",
    "    \n",
    "    # If you have enough data, can also subgrouping by 'year', so it will give as reference the same period (probably more attendible)\n",
    "\n",
    "    # List to contain imputed dataframes\n",
    "    imputed_dfs = []\n",
    "    true_values_list = []\n",
    "    imputed_values_list = []\n",
    "\n",
    "    for geo_value in df_geo_list:\n",
    "        # Extract the subgroup corresponding to the current value of 'geo'\n",
    "        group = df[df[geo_column] == geo_value].copy()\n",
    "\n",
    "        # Select a fraction of known data to be imputed\n",
    "        true_values = group.sample(frac=0.1)[col_to_test]\n",
    "        group.loc[true_values.index, col_to_test] = np.nan\n",
    "\n",
    "        # Remove unused categories in \"geo\" and \"country\" columns\n",
    "        group[geo_column] = group[geo_column].cat.remove_unused_categories()\n",
    "        group[other_cat_col] = group[other_cat_col].cat.remove_unused_categories()\n",
    "        \n",
    "        # --- MICE\n",
    "        \n",
    "        # Create the attribution kernel\n",
    "        kds = ImputationKernel(group, random_state=4)\n",
    "        \n",
    "        # Manually application\n",
    "        kds.mice(iterations=5, n_estimators=50, device='gpu') # verbose=2\n",
    "        # ---\n",
    "        \n",
    "        # Tuning parameters\n",
    "        # optimal_parameters, losses = kds.tune_parameters(dataset=0, optimization_steps=10)\n",
    "        # kds.mice(1, variable_parameters=optimal_parameters)\n",
    "        # print(optimal_parameters)\n",
    "        # ---\n",
    "        \n",
    "        # Get the imputed dataframe\n",
    "        df_imputed = kds.complete_data()\n",
    "        # ---\n",
    "        \n",
    "        # Restore original categories if necessary\n",
    "        df_imputed[geo_column] = pd.Categorical(df_imputed[geo_column], categories=df[geo_column].unique())\n",
    "        df_imputed[other_cat_col] = pd.Categorical(df_imputed[other_cat_col], categories=df[other_cat_col].unique())\n",
    "\n",
    "        # Add the imputed dataframe to the list\n",
    "        imputed_dfs.append(df_imputed)\n",
    "\n",
    "        # Recovers the imputed values\n",
    "        imputed_values = df_imputed.loc[true_values.index, col_to_test]\n",
    "\n",
    "        # Extend real and imputed value lists\n",
    "        true_values_list.extend(true_values)\n",
    "        imputed_values_list.extend(imputed_values)\n",
    "\n",
    "    # Combine imputed dataframes into a single dataframe\n",
    "    final_df = pd.concat(imputed_dfs)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(true_values_list, imputed_values_list))\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "\n",
    "    # Create a DataFrame with real and imputed values\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'True Values': true_values_list,\n",
    "        'Imputed Values': imputed_values_list\n",
    "    })\n",
    "\n",
    "    # Visualizza il DataFrame di confronto\n",
    "    display(comparison_df)\n",
    "    \n",
    "    return final_df, rmse, comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c34f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df, rmse, comparison_df = mice_forest_grouped(df, col_to_test='HICP(%)', grouping_column='geo', other_cat_col='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2422116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot compare the distribution by data and data with imputation\n",
    "kds.plot_imputed_distributions(wspace=0.2,hspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e35add",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213c01e",
   "metadata": {},
   "source": [
    "**<U>If you want to test and use the code**\n",
    "- download file in .py extension\n",
    "- save in specific folder\n",
    "- follow next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a376446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/utente/Desktop/---YOUR PATH ----/FILE_NAME.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3741486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "from (YOUR FOLDER LOCATION) import (FILE NAME WITHOUT EXTENTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cf7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all udf list\n",
    "all_udf = dir(FILE NAME WITHOUT EXTENTION)\n",
    "# Functions available\n",
    "udf = [f for f in all_udf if callable(getattr(FILE NAME WITHOUT EXTENTION, f)) and not f.startswith(\"__\")]\n",
    "udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from (FILE NAME WITHOUT EXTENTION) import (SPECIFIC UDF NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
